Metadata-Version: 2.4
Name: np_utils
Version: 0.1.0
Summary: So I don't rewrite the same code over and over and over
Requires-Python: >=3.9
Description-Content-Type: text/markdown
Requires-Dist: numpy>=1.20
Requires-Dist: pandas>=1.3
Requires-Dist: np-sheets@ git+https://github.com/EthanKato/np_sheet_utils@v0.1.0
Provides-Extra: spikeinterface
Requires-Dist: pynapple>=0.6; extra == "spikeinterface"
Requires-Dist: spikeinterface>=0.100; extra == "spikeinterface"
Requires-Dist: neuroconv>=0.4; extra == "spikeinterface"
Requires-Dist: psutil>=5.8; extra == "spikeinterface"
Requires-Dist: np-nwbmaker@ git+https://github.com/EthanKato/np_nwbmaker.git ; extra == "spikeinterface"
Provides-Extra: dev
Requires-Dist: pytest; extra == "dev"
Requires-Dist: black; extra == "dev"
Requires-Dist: flake8; extra == "dev"
Provides-Extra: all
Requires-Dist: pynapple>=0.6; extra == "all"
Requires-Dist: spikeinterface>=0.100; extra == "all"
Requires-Dist: neuroconv>=0.4; extra == "all"
Requires-Dist: psutil>=5.8; extra == "all"
Requires-Dist: np-nwbmaker@ git+https://github.com/EthanKato/np_nwbmaker.git ; extra == "all"
Requires-Dist: pytest; extra == "all"
Requires-Dist: black; extra == "all"
Requires-Dist: flake8; extra == "all"

# np_utils
So I don't rewrite the same code over and over and over

## Installation
```bash
pip install -e .
```

## API Reference

### Oversight Utils (`np_utils.oversight_utils`)

#### `get_rec_ids(column_name, condition, recordings_df=None)`
Get recording IDs that match a specific condition on a given column.

**Args:**
- `column_name` (str): Name of the column to check the condition against.
- `condition`: Either a value to match (for equality check) or a callable that takes the column and returns a boolean mask for complex conditions.
  - Simple: `condition=''` checks for empty strings
  - Complex: `condition=lambda col: col < some_date`
  - Complex: `condition=lambda col: (col > 10) & (col < 20)`
- `recordings_df` (pd.DataFrame, optional): Recordings dataframe. If None, loads from 'recordings' sheet.

**Returns:**
- `np.ndarray`: Array of recording IDs that match the condition.

#### `get_need_nwb(recordings_df=None)`
Get recording IDs that need NWB files generated.

**Args:**
- `recordings_df` (pd.DataFrame, optional): Recordings dataframe. If None, loads from 'recordings' sheet.

**Returns:**
- `np.ndarray`: Array of recording IDs that need NWB files.

#### `get_has_nwb(recordings_df=None)`
Get recording IDs that have NWB files generated.

**Args:**
- `recordings_df` (pd.DataFrame, optional): Recordings dataframe. If None, loads from 'recordings' sheet.

**Returns:**
- `np.ndarray`: Array of recording IDs that have NWB files.

### Job Utils (`np_utils.job_utils`)

#### `submit_job(script, rec_ids=None, python_executable="/userdata/ekato/miniforge3/envs/se2nwb/bin/python", queue="mind-batch", cores=8, memory_gb=16, log_dir="/tmp/job_logs", job_prefix="job", use_time=False, extra_args=None, executable=None, gpus=None, dry_run=False)`
Submit a job (or batch of jobs) to SGE queue using submit_job command.

**Args:**
- `script` (str | Path): Path to the script to run.
- `rec_ids` (List[str], optional): List of recording IDs to process. If provided, submits one job per rec_id, passing --rec-id to script.
- `python_executable` (str): Path to Python executable. Default: conda se2nwb environment.
- `queue` (str): Queue name (e.g., 'mind-batch', 'skull-gpu', 'pia-batch.q').
- `cores` (int): Number of CPU cores to request (ignored for GPU queues).
- `memory_gb` (int): Total memory in GB to allocate.
- `log_dir` (str | Path): Directory to store job output logs.
- `job_prefix` (str): Prefix for job names and log files.
- `use_time` (bool): If True, wraps command with /usr/bin/time -v for profiling.
- `extra_args` (List[str], optional): Additional arguments to pass to the script.
- `executable` (str, optional): Override Python with custom executable (e.g., 'matlab', 'pythonconda3'). If provided, python_executable is ignored.
- `gpus` (int, optional): Number of GPUs to request (use with GPU queues only).
- `dry_run` (bool): If True, prints command without executing.

**Returns:**
- `None`

#### `batch_submit(rec_ids, **kwargs)`
Convenience function for batch submissions.

**Args:**
- `rec_ids`: List of recording IDs to process
- `**kwargs`: All other arguments to pass to submit_job()

**Returns:**
- `None`

#### `submit_queue_throttled(items, submit_func, max_concurrent=2, check_interval=10, item_to_args=None, verbose=True)`
Submit jobs from a queue while maintaining a maximum number of concurrent jobs.

Monitors running jobs and submits new ones from the queue when capacity is available. Continues until all items in the queue have been submitted.

**Args:**
- `items` (List[Any]): Queue of items to process (e.g., rec_ids, file paths).
- `submit_func` (Callable): Function to call for each item. Should accept keyword args.
- `max_concurrent` (int): Maximum number of concurrent jobs allowed.
- `check_interval` (int): Seconds to wait between job count checks.
- `item_to_args` (Callable, optional): Function that converts an item to kwargs dict for submit_func. If None, assumes items are rec_ids and creates {'rec_ids': None, 'extra_args': ['--rec-id', item], 'job_prefix': f'job_{item}'}.
- `verbose` (bool): Print status messages.

**Returns:**
- `None`
**Examples:**
```python
run_queue = ["NP139_B2", "NP140_B1", "NP141_B3"]

# Option 2: Use the general throttled function for more control
def my_submit(**kwargs):
    nu.submit_job(
        script="/userdata/ekato/git_repos/np_utils/np_utils/spikeinterface/run_si_proc.py",
        python_executable="/userdata/ekato/miniforge3/envs/se2nwb/bin/python",
        queue="mind-batch",
        cores=9,
        memory_gb=256,
        log_dir="/data_store2/neuropixels/nwb/temp/SI_logs",
        use_time=True,
        **kwargs
    )

nu.submit_queue_throttled(
    items=run_queue,
    submit_func=my_submit,
    max_concurrent=2
)
```
#### `submit_rec_queue(rec_ids, script, python_executable, queue, cores, memory_gb, log_dir, job_prefix, use_time=False, max_concurrent=2, check_interval=10, extra_args_per_rec=None, **submit_kwargs)`
Convenience function for submitting a queue of recording IDs with throttling.

**Args:**
- `rec_ids`: List of recording IDs to process
- `script`: Path to script to run
- `python_executable`: Path to Python executable
- `queue`: Queue name
- `cores`: Number of CPU cores
- `memory_gb`: Memory in GB
- `log_dir`: Log directory
- `job_prefix`: Base prefix for job names (rec_id will be appended)
- `use_time`: Use /usr/bin/time wrapper
- `max_concurrent`: Max concurrent jobs
- `check_interval`: Check interval in seconds
- `extra_args_per_rec`: Optional function that takes rec_id and returns extra args list
- `**submit_kwargs`: Additional kwargs to pass to submit_job

**Returns:**
- `None`

**Examples:**

```python
run_queue = ["NP139_B2", "NP140_B1", "NP141_B3"]

# Option 1: Use the specialized rec_queue function
nu.submit_rec_queue(
    rec_ids=run_queue,
    script="/userdata/ekato/git_repos/np_se2nwb/SI/run_si_proc.py",
    python_executable="/userdata/ekato/miniforge3/envs/se2nwb/bin/python",
    queue="mind-batch",
    cores=9,
    memory_gb=256,
    log_dir="/data_store2/neuropixels/nwb/temp/SI_logs",
    job_prefix="SI",
    use_time=True,
    max_concurrent=2,
    check_interval=10
)
```
